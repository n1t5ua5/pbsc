## SYSTEM ANALYSIS & DESIGN: FALL 2025, PBSC


## Functional vs. Non-Functional Requirements:

Provide two examples of functional requirements for an e-commerce platform (e.g., "users can filter products by price") & two non-functional requirements (e.g., "99.9% uptime during peak hours"). How might conflicting priorities between these requirement types complicate system design?

Functional requirements describe the specific operations taking place on a regular basis inside a web application, such as creating an account & logging in or adding items to a shopping cart. Non-functional requirements are slightly different in that they measure performances of a system, like ensuring every users information is encrypted or that the platform is scalable & can handle a high amount of traffic when necessary [1]. For any e-commerce platform, functional requirements are similar to the spine in our body, whereas they uphold the framework of what we are while still maintaining a level of flexibility. Non-functional requirements are more akin to something like how many heartbeats we have per minute. Both requirements are vastly different but similar in that we need to implement & uphold all of these things unitarily if we want to maintain robust, transparent systems.

Yet, they can still come into conflict with one another, like sacrificing page load speed for the sake of only using high resolution images. Though different, using them together in the system design process equates to applications that are fully capable of growing when the appropriate metrics have been met because they provide context into not just the things that have occurred already, but for future events as well. This constant need to balance what the system does with how well it performs makes system design a complex process of trade-offs & priorities.

Reference

[1] GeeksForGeeks. (2020, April). Functional vs. Non Functional Requirements: https://www.geeksforgeeks.org/software-engineering/functional-vs-non-functional-requirements/

-----
-----

## 1. Data Dictionary Components:
A data dictionary documents data elements, flows, stores & entities. What specific information should each entry include (e.g., name, type, format)? How does this documentation reduce ambiguity during system development? Illustrate with an example for a data flow like "Payment Confirmation."

A data dictionary is basically the rulebook for all of the data in a particular system. For every single piece of data (like an account name or transaction date), each entry must clearly document its Name (for identification purposes), its Type (like an integer or a date from the calendar) & its Format (like how long someone's input can be or a system's physical appearance). Data dictionaries also include a description explaining what the actual data represents, a source indicating where it came from & a list of all the smaller data elements it contains. Writing out this documentation is imperative for proper system design because it reduces ambiguity by giving everyone access to a single, immutable definition for every data element.

For example, a data flow like "Payment Confirmation" should be defined as the movement of successful payment data. A good system analyst should clearly list all essential data elements, like the customer ID, the amount paid by the customer & the payment date, to ensure that everyone uses the same data fields & formats.
References.

References.

Tilley, Scott R., (2023). Systems Analysis & Design via Cengage.

UC Merced Library, (2023). What is a Data Dictionary? via Library.ucmerced.edu: https://library.ucmerced.edu/data-dictionaries

-----
-----

## 4. CASE Tools & Model Integrity:
CASE tools automate tasks like code generation & diagram synchronization. What risks emerge if a class diagram's associations conflict with its sequence diagram? How can analysts mitigate these risks during iterative design?

The first & foremost risk in having a project's class diagram conflict with its connected sequence diagram is the likely loss of general functionality. For example, if the class diagram specifies how classes interact with each other or how they could store data, the sequence diagram will completely overlook these crucial features if they're not included. In turn, if these components are overlooked during the initial design process, the entire program may fail to function properly upon launching - especially without thorough testing. Another major risk is how the same lack of components specified in the design phase could lead to incomplete or inconsistent code generated by the CASE tools you're using, for the same reasons listed previously.

In short, conflicts between different UML diagrams will compromise model integrity & inconsistencies ensure that whatever type of code is generated by any sort of CASE tool will assuredly be flawed. System analysts can work to mitigate these risks by upholding thorough review processes to validate every component & model created + all of the different associations between various UML diagrams. Automated review processes should also be designed to include iterative design validation to ensure that every single sequence is connected to the class diagram. Both of these things will help reduce the number of potential errors incurred.

References.

[1] Bhuyan, A, (2023). Comprehensive Overview of CASE Tools: Streamlining Software Development via faun.pub: https://faun.pub/comprehensive-overview-of-case-tools-streamlining-software-development-e367b156cb18
[2] PyCharm, (n.d.). UML Class Diagrams via jetbrains.com: https://www.jetbrains.com/help/pycharm/class-diagram.html

-----
-----

## Usability vs. Aesthetics:
The chapter states that effective UI design must balance usability with visual appeal. Provide two examples where prioritizing aesthetics might compromise usability & vice versa. How can systems analysts mitigate these trade-offs during the design process?

When aesthetics are prioritized foremost, usability is usually sacrificed at its expense. For example, designers might choose a low-contrast color scheme for a minimalist design appeal, but this could severely compromise readability & accessibility for many users. Another common aesthetic shortcoming that I've seen almost everywhere online in recent years is the inability of design teams to adequately prepare for accessibility features like 'dark-mode' browser extensions, which reverse color schemes & can alter interfaces with unintended consequences if they are not built to handle this sort of change. The same kind of loss accessibility could be done by using abstract images instead of commonly found one's. 

On the other side of things, focusing purely on usability can compromise aesthetics. One example that is particularly bothersome to me are annoying popups such as 'tooltips' which are supposed to add functionality but ultimately clog the pages they're on. Likewise, only using generic images such as things found on Google could hinder users from returning to the site your managing. To properly navigate these trade-offs, systems analysts must rely on usability testing & iterative design. They must first establish a functional foundation by enforcing non-negotiable accessibility standards (like adequate color contrast) as mandatory design parameters. Then they should build prototypes, test them with real users to catch functional problems caused by aesthetic choices & lastly properly update the project to officiate any changes.

References.
[1] Priodev Media. (2025, March). Balancing Aesthetics & Usability in UI Design via priodev.com: https://www.priodev.com/ui-design-aesthetics-usability/

-----
-----

## 2. Normalization Trade-offs:
While normalization reduces redundancy, it can complicate query performance. Describe a scenario (e.g., high-transaction e-commerce) where denormalization might be justified. How can analysts balance efficiency with data integrity?

Another scenario that exists in the real world where denormalization would be justified is, like how an analyst could be working to improve user experience through enhanced query performance. However, this could result in tables being improperly split if not monitored accordingly, but it can be handled correctly by creating a fact table, where any key field is inserted into the main table being used. The best way that any analyst can balance efficiency with data integrity is by ensuring all the user needs are being met by any query being made.

Another way to accomplish this is through controlled redundancy, which is something analysts can employ to reduce the amount of errors that could potentially be incurred. This is also referred to as materialized or indexed views. These sorts of pre-calculated tables can store the results of complex join statements & are automatically refreshed on a set schedule, so any updates that need to happen are scheduled to take place regularly. This is essential for working in data warehouse environments where stability is usually prioritized over real-time consistency. Maintaining the original, normalized tables for transactional purposes can serve as a reliable, single source of truth for any type of future update or insertion.

References.
[1] Valorem Reply. (2025, June). What Is Data Normalization? via valoremreply.com: https://www.valoremreply.com/resources/insights/blog/what-is-data-normalisation/
[2] Wickramasinghe, S. (2023, March). Data Denormalization: The Complete Guide | Splunk-Blogs via splunk.com: https://www.splunk.com/en_us/blog/learn/data-denormalization.html

-----
-----

## 3. Edge Computing vs. Centralized Cloud:
How does edge computing reduce latency in real-time applications (e.g., autonomous vehicles)? Discuss scenarios where hybrid architectures (combining edge & cloud) optimize performance & cost-efficiency.

Edge computing allows the computers we use to access the data they need from servers that are geographically closer to them, while the centralized cloud geographically limits itself to either one specific location or a few specific locations, depending on the size of the data center. If a cloud data center is located on the East Coast of the United States, it will likely always stay there. Whereas with edge computing, you can at least handle the data for the people towards the nearest server when necessary, which helps reduce latency in real-time applications. More specifically, with autonomous vehicles, onboard computers make instant decisions without sending data to the cloud.

While the costs of establishing a reliable, hybrid architecture are extremely high & not necessarily feasible for any small mover in the marketplace, its nature by design will allow its users to adapt flexibly to change. In the event there's ever a 'traffic jam' around peak-user time, the system will be able to split users into groups according to their needs. This type of dynamic workload placement ensures that latency-sensitive tasks are prioritized at the edge while large-scale data analysis is handled by the cloud for cost savings. This means quick tasks stay local, while less urgent tasks are sent to the cloud to save money. If the costs are not prohibitive, the speed of the edge & scale in the cloud combined allows for a more flexible structure that's sure to keep more people & groups online in the event of a disaster.

References.
[1] Scale Computing. (2023). What's the Difference Between Edge Computing & Cloud Computing? via scalecomputing.com: scalecomputing.com/difference-between-edge-computing-cloud-computingLinks to an external site. 
[2] Kurduban, V. (2024). Edge Computing vs Cloud Computing: Differences & Relationship | Digi International via digi.com: https://www.digi.com/blog/post/edge-computing-vs-cloud-computing

-----
-----

## 3. Edge Computing vs. Centralized Cloud:
How does edge computing reduce latency in real-time applications (e.g., autonomous vehicles)? Discuss scenarios where hybrid architectures (combining edge & cloud) optimize performance & cost-efficiency.

Edge computing allows the computers we use to access the data they need from servers that are geographically closer to them, while the centralized cloud geographically limits itself to either one specific location or a few specific locations, depending on the size of the data center. If a cloud data center is located on the East Coast of the United States, it will likely always stay there. Whereas with edge computing, you can at least handle the data for the people towards the nearest server when necessary, which helps reduce latency in real-time applications. More specifically, with autonomous vehicles, onboard computers make instant decisions without sending data to the cloud.

While the costs of establishing a reliable, hybrid architecture are extremely high & not necessarily feasible for any small mover in the marketplace, its nature by design will allow its users to adapt flexibly to change. In the event there's ever a 'traffic jam' around peak-user time, the system will be able to split users into groups according to their needs. This type of dynamic workload placement ensures that latency-sensitive tasks are prioritized at the edge while large-scale data analysis is handled by the cloud for cost savings. This means quick tasks stay local, while less urgent tasks are sent to the cloud to save money. If the costs are not prohibitive, the speed of the edge & scale in the cloud combined allows for a more flexible structure that's sure to keep more people & groups online in the event of a disaster.

References.
[1] Scale Computing. (2023). What's the Difference Between Edge Computing & Cloud Computing? via scalecomputing.com: https://www.scalecomputing.com/resources/what-is-the-difference-between-edge-computing-and-cloud-computing
[2] Kurduban, V. (2024). Edge Computing vs Cloud Computing: Differences & Relationship | Digi International via digi.com: https://www.digi.com/blog/post/edge-computing-vs-cloud-computing

-----
-----

## Testing for Reliability:
Compare the objectives of unit testing & system testing. How might inadequate integration testing cascade failures in a mission-critical system (e.g., healthcare records)? What role do test cases play in mitigating these risks?

Unit testing is a form of testing used by software developers to test if small, specific pieces of code inside of a program are working or not. System testing has a much wider scope & pertains to validating an entire system against specified user requirements. Both are proven methods for developers to test if the code they wrote is truly working or not, other than just seeing a change in the graphical user interface (GUI). Improper integration testing can lead to cascading failures when modules fail to exchange the data they require. Failures in a mission critical system must be addressed above any other concerns because if the system is unable to perform correctly, the implications will be a total loss of system functionality.

Test cases help mitigate these risks because they address these sort of specific concerns as long as the developer designs them correctly. Unit tests & system tests alike both contain test cases, which contain explicit, step-by-step instructions on how a program should run. It would be impossible to check if the code we wrote worked or not without testing test cases. If a test case is designed to only accept a very narrow piece of data, like if the CSS is being updated properly or not, as opposed to if a user can access their account, there will most certainly be failures down the line. 

References.
[1] Pittet, S. (n.d.). The Different Types of Testing in Software. | Atlassian via atlassian.com: https://www.atlassian.com/continuous-delivery/software-testing/types-of-software-testing
[2] Brush, K. (2024). What is a Test Case? | Tech Target via techtarget.com: https://www.techtarget.com/searchsoftwarequality/definition/test-case

-----
-----

## 4. Post-Implementation Evaluation:
Post-implementation reviews assess metrics like error rates & user satisfaction. Identify two quantitative & two qualitative metrics for evaluating a retail inventory system's performance. How might these metrics inform future scalability decisions?

Two important quantitative measures that every system analyst should check in a retail inventory system are the stockout rate, which displays how often items go out of stock, & the overstock rate, which displays how many extra items are present in the inventory system. All businesses need to ensure the proper rates at which items are coming or going from the inventory, so they can make the most informed decisions about how many new items will be needed for the next shipment of goods. Without this sort of knowledge, a business can unknowingly order too many or even too few of the items they need when restocking their shelves. 

A major qualitative measure that I would analyze would be employee satisfaction, because everyone has to overcome this hurdle if they're trying to scale a business. Since it's nearly unavoidable, every business should consider this qualitative measure. While it may seem like causing friction at first, without knowing exactly how employees stand on certain issues, they could potentially be repeating the same mistakes or be developing burnout from working too many hours. Another similar qualitative measure to consider analyzing is overall customer satisfaction. The results can help pinpoint exactly where a business is going wrong & how they can improve themselves. Both of these measures can be fulfilled by giving either employees or customers access to a survey link in their pay stubs or payment receipts. 

References.
[1] Ordanic, D. (2024, April). Mastering Quantitative & Qualitative Metrics for Success. | via insightful.io: https://www.insightful.io/blog/qualitative-vs-quantitative
[2] Omniful. (2025). Best Practices & Qualitative Aspects of Inventory Analysis. | via omniful.ai: https://www.omniful.ai/blog/inventory-analysis-practices-and-insights

-----
-----

## 3. Ethics in Security Design:
The chapter notes that "security measures can conflict with user privacy." Provide an example (e.g., employee monitoring software) where ethical considerations influence access control policies. How can analysts navigate stakeholder concerns while maintaining compliance?

Ethical considerations should be taken into account before starting any new security project. That way, we can establish a healthy habit of asking pertinent security questions before it's too late. One way ethics influences access control policies is through file-sharing policies, like with programs such as Microsoft OneDrive or Google Drive. For example, a managerial-level employee creates a document named systemAnalysis2025.docx. That employee can then grant other coworkers the ability to view or even edit the same file. The original owner of the file is also able to revoke said access at any time.

Network intrusion detection systems (NIDS) are another good example for this discussion. Their security requires deep packet inspection to analyze data & detect malicious threats, but this process of scrutinizing user communications raises privacy concerns too. This means that for NIDS, access control has to be balanced very carefully. System analysts can navigate stakeholder issues by necessitating these sorts of privacy measures to be included in official company documentation that's available to the public, so any employee or prospective employee can always have access to them at any time as well. By being transparent about security practices & limiting data collection to what is strictly necessary, system analysts can safeguard & build trust throughout the organizations they're working in.

References.
[1] Augusta University. (2024). Cybersecurity Ethics: What Cyber Professionals Need to Know. | via augusta.edu: https://www.augusta.edu/online/blog/cybersecurity-ethics
[2] Cleveland State University. (2024, September). Understanding Cybersecurity Ethics & Navigating Moral Complexities. | via csuohio.edu: https://onlinelaw.csuohio.edu/blog/understanding-cybersecurity-ethics-and-navigating-moral-complexities/